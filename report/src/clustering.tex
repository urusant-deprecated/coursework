\section{Кластеризация}
\subsection{Оценка эффективности кластеризации}
Чтобы иметь возможность сравнивать различные алгоритмы кластеризации, нужно привести способ оценивания их эффективности. В данной работе для тестирования использовалось подмножество популярной коллекции Reuters-21578, а именно, были выбраны все тексты из нее, которые принадлежат ровно одной из списка категорий \{acq, crude, earn, grain, interest, money-fx, ship, trade\}. Таким образом, выделялось 8 кластеров.
Для каждого из тестируемых алгоритмов было и для каждого варианта функции из списка выше находится кластеризация, то есть разбиение множества документов на несколько групп, обозначенных метками. Кроме того, была также использована 5я функция, считающаяся стандартной для подобных задач. Она вводится с целью сравнить предлагаемый алгоритм с уже существующими. Далее, с целью вычисления эффективности решения для каждого из найденных кластеров находится самая часто встречающаяся категория, которая и полагается основной для данного кластера, и затем считается доля ошибок, то есть количество документов, категория которых не совпадает с категорией их кластера, деленное на общее количество документов. \newline
Теперь рассмотрим алгоритмы, использованные для кластеризации, а именно, следующие:
\begin{itemize}
\item Модификация k-means
\item K-medoids
\item Spectral clustering
\end{itemize}
Отметим, что в первых 2х алгоритмах требуется не похожесть, а, наоборот, расстояние между документами, то есть некоторая величина, которая тем больше, чем меньше документы похожи. В качестве такой величины удобно использовать значение $\frac{1}{similarity}$.
\subsection{Модификация k-means}
Классический k-means заключается в следующем. Пусть есть сколько-то точек в некотором линейном пространстве. Сначала выбираем случайно k точек - центроиды наших кластеров. Далее итеративно выполняем следующее:
\begin{itemize}
\item Пересчитываем кластеры, а именно, для каждой точки множества находим ближайший центроид.
\item Пересчитываем центроиды, то есть для каждого из полученных кластеров находим новый центроид как центр масс кластера.
\end{itemize}
Однако, проблема заключается в том, что у нас нет самих координат точек, а только расстояния между ними (расстояния считаются как $\frac{1}{similarity}$). В связи с этим модифицируем алгоритм следующим образом: будем выбирать изначальные центроиды как элементы нашего множества, а при пересчете будем выбирать тот элемент множества, от которого сумма расстояний до точек кластера минимальна. Таким образом, нам не требуется знать координаты точек, а только попарные расстояния между ними. \newline
Ниже приведены результаты работы этого алгоритма для каждого способа нахождения похожести текстов при 60 итерациях.
\begin{itemize}
\item Для $f(a, b) = \frac{a + b}{2}$ результат равен $0.523162$.
\item Для $f(a, b) = min(a, b)$ результат равен $0.469608$.
\item Для $f(a, b) = max(a, b)$ результат равен $0.516789$.
\item Для $f(a, b) = \sqrt(ab)$ результат равен $0.514338$.
\item Для стандартного способа результат равен $0.655392$.
\end{itemize}

\subsection{K-medoids}
Идея алгоритма похожа на k-means, но отличается способом пересчета центроидов. Изначально мы инициализируем кластеризацию, выбрав k случайных центроидов и отнеся каждый из остальных элементов в кластер к ближайшему центроиду. Далее, на каждой итерации для каждого документа пробуем поменять его с центром его кластера. Для полученной конфигурации пересчитываем кластеры (т.е. для каждого документа выбираем ближайший центр) и вычисляем некоторую величину, которая показывает, насколько хороша полученная кластеризация. В данном случае удобно использовать сумму расстояний от каждого документа до его центра. Таким образом, чем меньше эта величина, тем лучше кластеризация.
Итак, для каждой из таких замен мы посчитали, насколько улучшится (и улучшится ли вообще) кластеризация. Теперь просто выберем ту, которая дает самое большое улучшение, и применим. \newline
Количество итераций в этом алгоритме, в отличие от k-means, не выбирается фиксированным, а алгоритм выполняется до тех пор, пока у нас получается найти улучшающую замену. При выбранных ограничениях (~8000 элементов) на практике требуется всего несколько десятков итераций, поэтому алгоритм достаточно быстро завершает работу.
Ниже приведены результаты работы этого алгоритма так же, как это было сделано в предыдущем пункте.
\begin{itemize}
\item Для $f(a, b) = \frac{a + b}{2}$ результат равен $0.545833$.
\item Для $f(a, b) = min(a, b)$ результат равен $0.484681$.
\item Для $f(a, b) = max(a, b)$ результат равен $0.575$.
\item Для $f(a, b) = \sqrt(ab)$ результат равен $0.546569$.
\item Для стандартного способа результат равен $0.648284$.
\end{itemize}
\subsection{Spectral clustering}
Этот алгоритм был выбран как самый подходящий из модуля scikit-learn для python. Его суть состоит в нахождении координат точек в k-мерном пространстве по матрице похожести, и после этого применяется k-means к полученным точкам.
Ниже приведены результаты работы этого алгоритма.
\begin{itemize}
\item Для $f(a, b) = \frac{a + b}{2}$ результат равен $0.6867647058823529$.
\item Для $f(a, b) = min(a, b)$ результат равен $0.6938725490196078$.
\item Для $f(a, b) = max(a, b)$ результат равен $0.6843137254901961$.
\item Для $f(a, b) = \sqrt(ab)$ результат равен $0.6922794117647059$.
\item Для стандартного способа результат равен $0.6931372549019608$.
\end{itemize}
