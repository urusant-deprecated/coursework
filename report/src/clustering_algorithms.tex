\section{Аффинные алгоритмы кластеризации}
\subsection{Модификация k-means}
Классический k-means описан, например, в [3] и заключается в следующем. Пусть есть сколько-то точек в некотором линейном пространстве. Сначала выбираем случайно k точек - центроиды наших кластеров. Далее итеративно выполняем следующее:
\begin{itemize}
\item Пересчитываем кластеры, а именно, для каждой точки множества находим ближайший центроид.
\item Пересчитываем центроиды, то есть для каждого из полученных кластеров находим новый центроид как центр масс кластера.
\end{itemize}
Однако, проблема заключается в том, что у нас нет самих координат точек, а только расстояния между ними (расстояния считаются как $\frac{1}{similarity}$). В связи с этим модифицируем алгоритм следующим образом: будем выбирать изначальные центроиды как элементы нашего множества, а при пересчете будем выбирать тот элемент множества, от которого сумма расстояний до точек кластера минимальна. Таким образом, нам не требуется знать координаты точек, а только попарные расстояния между ними. \newline

\subsection{K-medoids}
Идея алгоритма похожа на k-means, но отличается способом пересчета центроидов. Изначально мы инициализируем кластеризацию, выбрав k случайных центроидов и отнеся каждый из остальных элементов в кластер к ближайшему центроиду. Далее, на каждой итерации для каждого документа пробуем поменять его с центром его кластера. Для полученной конфигурации пересчитываем кластеры (т.е. для каждого документа выбираем ближайший центр) и вычисляем некоторую величину, которая показывает, насколько хороша полученная кластеризация. В данном случае удобно использовать сумму расстояний от каждого документа до его центра. Таким образом, чем меньше эта величина, тем лучше кластеризация.
Итак, для каждой из таких замен мы посчитали, насколько улучшится (и улучшится ли вообще) кластеризация. Теперь просто выберем ту, которая дает самое большое улучшение, и применим. \newline
Количество итераций в этом алгоритме, в отличие от k-means, не выбирается фиксированным, а алгоритм выполняется до тех пор, пока у нас получается найти улучшающую замену. При выбранных ограничениях (~8000 элементов) на практике требуется всего несколько десятков итераций, поэтому алгоритм достаточно быстро завершает работу.

Более подробно этот алгоритм описан в [4] и [5].

\subsection{Spectral clustering}
Этот алгоритм был выбран как самый подходящий из модуля scikit-learn для python. Его суть состоит в нахождении координат точек в k-мерном пространстве по матрице похожести, и после этого применяется k-means к полученным точкам. Для дальнейшего изучения см. [6].