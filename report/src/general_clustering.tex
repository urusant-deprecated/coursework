\section{Формальная постановка задачи кластеризации}

Самая общая постановка задачи кластеризации документов, согласно [1], следующая. Дается некоторый набор документов $D = \{D_1, \dots, D_n\}$, и желаемое количество кластеров $K$. Требуется найти такое отображение $\gamma: D \rightarrow \{1, \dots, K\}$, которое максимизирует некоторую целевую функцию, показывающую, насколько кластеризация хорошая.

Большинство алгоритмов кластеризации можно разделить на 2 основных типа. Первый из них, \emph{объектно-признаковый}, заключается в том, что документы тем или иным образом представляются в многомерном векторном пространстве, где каждая координата является \emph{признаком}, и значение этой координаты у каждого документа говорит о том, насколько этот признак выражен в данном документе. Плюсом этого способа представления является достаточно простая математическая интерпретация множества документов. Однако, задача по выбору подходящего набора признаков и подбора значений координат часто бывает очень сложной, и поэтому вместо этого используются \emph{аффинные} алгоритмы кластеризации. Их суть заключается в том, что задача решается на основе матрицы похожести, выраженной действительным числом для каждой пары документов.

Рассмотрим теперь некоторые аффинные алгоритмы, использованные для кластеризации, а именно, следующие:
\begin{itemize}
\item Модификация k-means
\item K-medoids
\item Spectral clustering
\end{itemize}
Отметим, что в первых 2х алгоритмах требуется не похожесть, а, наоборот, расстояние между документами, то есть некоторая величина, которая тем больше, чем меньше документы похожи. В качестве такой величины удобно использовать значение $\frac{1}{similarity}$.